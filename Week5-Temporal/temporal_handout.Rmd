---
title: 'Handout: Temporal Diversity'
author: 'Z620: Quantitative Biodiversity, Indiana University'
date: "February 10, 2017"
header-includes:
   - \usepackage{array}
output: pdf_document
geometry: margin=2.54cm
---

## OVERVIEW

In this handout, we will explore some basic aspects of temporal diversity. 
Biological systems are inherently variable thorugh time.
While some of this variation is stochastic, ecological systems can change though time becasue of abitoic (e.g., rising temperature, increasing CO~2~, drought) or biotic conditions (e.g., invasive species, disease). 
While some biodiversity scientists focus on processes that occur on contemporary scales (minutes, days, weeks, years), other scientists are interested in what drives patterns of biodiversity in the fossil record over geologic time scales. 
In this module, we will first introduce you to a new ecological data structure, the time-by-site-species matrix.
You will learn how to manage and manipulate this type of data structure using some new R packages. 
We will then introduce the basics of time series analysis, which will be followed by sections dealing with repeated-measures analysis of variance (RM-ANOVA), temperal beta diversity (i.e., turnover), and synchrony. 

## 1) SETUP

### Retrieve and Set Your Working Directory

```{r, results = 'hide'}
rm(list=ls()) 
getwd() 
setwd("~/GitHub/QuantitativeBiodiversity/QB-2017/Week5-Temporal/") 
```

### Install Packages

We will be using several packages in this week's module. 
Let's load them now. 
The `require()` function in `R` returns `TRUE` if the package was successfully loaded or `FALSE` if the package failed to load. 
This `for` loop loads each package and installs the package when `require()` returns `FALSE`.

```{r, results = 'hide', message = FALSE, warning = FALSE} 
package.list <- c('vegan', 'tidyr', 'dplyr', 'codyn', 'ggplot2', 'cowplot', 'MullerPlot', 'RColorBrewer', 'reshape2', 'lubridate', 'TTR', 'xtable', 'multcomp', 'pander', 'png', 'grid', 'tseries')
for (package in package.list) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package)
    library(package, character.only=T)
  }
}
```

## 2) LOADING DATA

To learn about topics of temporal biodiversity, we will be using the long-term monitoring dataset of rodents from the Chihuahuan Desert ecosystem near Portal, Arizona.
Known as the "Portal Project" <http://portal.weecology.org/>, this biodiversity study was initiatied in the late 1970s by Jim Brown and colleagues to look at species interactions, specifically competition between rodents and ants for plant seeds.
Early experiments focused on the exclusion of rodents with fencing and trapping, which led to an increase in the ant population size as well as changes in plant species composition.
These findings inspired Brown and colleagues to construct an improved experiment that continues today. 
The Portal Project monitors 24 replicated experimental plots, each 0.25 ha in area (50 X 50 m).
In total, 4.8 kilometers of fencing is used in the Portal experiment!

Let's take a look at the data:

```{r}
portal <- read.table("data/combined.csv", sep = ",", header = TRUE)
```

The version of the data that we are working with spans from 1977 - 2002. 
During this period, individual rodents were captured from plots ("plot_id"), identified to species ("species_id"), which led to the creation of a "record_id" with an associated date (day, month, and year).
In addition, the sex, size ("hindfoot_length" and "weight") and taxonomic identity ("genus" and "species") of each animal was recorded. 
All of this was done in five experimentally replicated treatments (see Figure): 

1) Controls - fencing around plots does not exclude rodents (plot_id: 2, 4, 8, 11, 12, 14, 17, 22) 

2) Long-term Krat - long-term exclusion of Kangaroo rats (*Dipodomys merriami*) (plot_id: 3, 15, 19, 21) 

3) Short-term Krat - short-term exclusion of Kangaroo rats (*Dipodomys merriami*) (plot_id: 6, 13, 18, 20) 

4) Rodent Exclosure - exclusions of all rodents (plot_id: 5, 7, 10, 16, 23, 24) 

5) Spectab exclosure - exclusion of Banner-tailed kangaroo rat (*Dipodomys spectabilis*) (plot_id: 1, 9) 

```{r, fig.width = 8, fig.height = 3.75, echo=FALSE, fig.align='center', echo=FALSE}
img <- readPNG("data/portal.png")
grid.raster(img)
```

## 3) MANAGING AND MANIPULATING DATA

The Portal Project is a great example of a long-term biodiversity project. 
The success of the Portal Project is based not only on the generation of a lot of quality data, but also the organization and proper management of these data. 
One thing you may notice when looking at `portal` is that it has a lot of observations, almost 35,000.
These data are entered in *long format*, which means that, as described above, each row represents a unique observation. 
The long format is a convenient way to enter data that reduces entry errors.
However, you need to manipulate the data for visualization and statistical analysis. 
For example, many R packages and functions will want you to organize your data in *wide format* where, for example, different sampling time points would be represented in columns.

In the following sections, we will demonstate how to manipulate the `portal` data set to carry out different types of analyses using functions from the `dplyr` and `tidyr` packages, which were designed for transforming, subsetting, and summarizing tabular data.
The functions contained in these two packages (and others) are preferred ways for manipulating data in R compared to other control-flow statements (e.g., for loops and while loops) that are commonly used in other programming languages. 

First, let's use the `unite()` function from the `tidyr` package to create new columns that contain information in other columns. 

```{r}
# Make a date column that contains year, month, and day
portal <- unite(portal, col = date, c(year, month, day), sep = "-", remove = FALSE)

# Make a taxon column that contains genus and species names
portal <- unite(portal, col = taxon, c(genus, species), sep = "_", remove = FALSE)
```

Now, we are going to use `dplyr` to create a time-by-species matrix. 
To do this, we are going to use a new operator referred to as "pipes" (%>%). 
Pipes allow output from one function to be used as input for another function in the same line of code.
When using pipes, you use the output from the function to left to feed into the next function to the right. 
To build the time-by-species matrix, we we also be using a `dplyr` function called `group_by`, which splits the dataset up using a single or multiple variables.
After this, `dplyr` allow one to apply functions to the split datset using functions like `summarise()`, which can return summary statistics (e.g., mean, min, max).
You can also use the `spread()` function from `tidyr` on the split dataset to convert a parts of a long format dataset to a wide format dataset. 

```{r}
# `count()` sums individuals in each unique taxon, while
# maintaining the grouping by year and plot_id;
# fill argument assigns value of 0 if there are no values for a combination
time.by.species <- group_by(portal, year, plot_id) %>% 
  count(taxon) %>% spread(key = taxon, value = n, fill = 0)

# we can now create many site- or time-by-species matrices from this structure
dplyr::filter(time.by.species, year == 1984) # return 1984 site by species
dplyr::filter(time.by.species, plot_id == 5) # return plot5 time by species


# Convert tidyr object to a dataframe
time.by.species <- as.data.frame(time.by.species)
```

## 4) TIME SERIES ANALYSIS - AUTO-REGRESSIVE MOVING AVERAGE (ARMA) MODELS
Because many questions in biodiversity science involve time, we are going to provide a brief overview of time series analysis.
Time series analysis involves statistical tools for detecting and decomposing trends in temporal data, and distinguishing this from other sources (e.g., error).
One of the most basic ways of analyzing a time series is with auto-regressive moving aveverage (ARMA) models.
The autoregressive component of an ARMA model uses regression to obtain *coefficients* that predict current observations using previous or "lagged" observations from a time series. 
The moving average component of an ARMA model uses multiple regression to recover *error terms* of the current and prior observations. 
Ultimately, ARMA approaches can help identify trends in data and help to make forecasts about future observations. 
As such, ARMA is commonly used in ecology, but also in business and economics.
In this section we walk through the basics of ARMA, but also introduce some other commonly used techniques for the preliminary analysis of a time series. 
Specifically, we provide a primer of time series analysis using the abundance of rodents in a single site of the Portal Project.
The Chihuahuan Desert sits in a rain shadow created by the Sierra Madre mountains.
As a result, the Portal site is dry with rain falling primarily during the months of June through October.
We will try to detect a signal of this seasonality in our dataset, while testing for long-term trends and forecasting rodent densities into the future.
Let's get started by manipulating our data using some of the tools that we introduced above from the `dplyr` and `tidyr` packages.

### Data wrangling

```{r}
# Create a time-by-species matrix that includes year, month, and plot_id
time.by.spec.2 <- group_by(portal, year, month, plot_id) %>% count(taxon) 

# Create a seasonality variable using month number
time.by.spec.2$season <- NA
time.by.spec.2$season <- time.by.spec.2$month %in% c(6:10)

# Rainy seasons are June - October
time.by.spec.2$season <- ifelse(time.by.spec.2$season == TRUE, "rain", "norain")

# Group the data by year and season
group_by(time.by.spec.2, year, season)

# Pick a plot and then calculate rodent abundance for each season with a year 
abund <- filter(time.by.spec.2, plot_id == 2) %>% 
  group_by(year, season) %>% 
  count(wt = n)

# Correct abundances to express on a per hectare basis
abund$nn <- abund$nn * 4
```

### Smoothing

One technique that is commonly used, mostly for visualization purposes, is *smoothing*. 
These types of techniques are used as low-pass filters that effectively remove noise from a data set, and aid in visualizing signal in your dataset.
In the follow chunk of code we will use a simple moving average procedure from the `TTR` package. 
The function `sma()` calculates the arithmetic mean of the time series over the past *n* observations.
This unweighted, moving average approach smooths data with increasing *n*. 
Try out the code below and play around with *n*.

```{r{}} 
ab.ts.sm <- SMA(ab.ts, n = 5)
plot.ts(ab.ts.sm, ylab = "Rodent Abundance (#/hectrare)", xlab = "Time (year)", las = 1)

ab.ts.hw <- HoltWinters(ab.ts, beta = FALSE, gamma = FALSE)
ab.ts.hw$fitted
plot(ab.ts.hw, xlab = "Time (year)", las = 1, main = NA)
```

### Decomposing the time series
As mentioned above, we sampled subsampled the `portal` data set so that it included seasonal variation that could be driven by patterns of precipitation. 
In the following section we will use the `decompose()` function which decomposes the time series so that we can assess contributions from the overal trend, seasonality, and radom processes.
If seasonal trends are a nuisance in your study, there are ways to remove them, as done in the chunk below. 

```{r{}
ab.comp <- decompose(ab.ts) # moving average decomposition
plot(ab.comp) 

ab.adj <- ab.ts - ab.comp$seasonal # remove seasonality
plot.ts(ab.adj, ylab = "Rodent Abundance (#/hectrare)", xlab = "Time (year)", las = 1)
```

### ARMA
Now, we are going to use a set of time series functions in the R base package along with some functions from the `tseries` package. 
The first thing we will do is create a time series data set, which identifies the start time and seasonality of our sampling.

```{r}
# Define the time series and frequency (i.e., number of observations per year)
ab.ts <- ts(abund$nn, frequency = 2, start = c(1977, 2))

# Let's look at a plot of the data
plot.ts(ab.ts, ylab = "Rodent Abundance (#/hectrare)", xlab = "Time (year)", las = 1)
```

We can see from the plot that there may be some trends in the data.
If the mean, variance, or covariance change as a function of time, then we are likely to be violating the assumption of **stationary**. 
A stationary series is one where the mean, variance, and covariance is not related to time.
If the assumption of stationarity is not met, corrective measures should be taken, which could invovle transforming or differencing (t - 1) the data. 
Here we will use the `adf.test()` function in the `tseries` package, which implements the Augmented Dickey-Fuller Test.  
This is a commonly applied statistic that tests stationarity in a time series.
We will be testing the null hypothesis that time series is non-stationary, which means we will condlude that a time series is stationary if the p-value > 0.05.

```{r}
adf.test(ab.ts.diff, alternative = "stationary") # small p = non-stationary
```

The Dickey-Fuller test confirms that our time series does not meet the assumption of stationarity.
Let's try differencing the time series to see if this helps. 

```{r}
ab.ts.diff<-diff(ab.ts)
adf.test(ab.ts.diff, alternative = "stationary") 
plot.ts(ab.ts.diff, ylab = "Rodent Abundance (#/hectrare)", xlab = "Time (year)", las = 1)
```

One of thing that we will do know is to look at the lags in our timeseries using 
the *autocorrelation function (ACF)* and the *partial autocorrelation function (PACF)*.
The trends in these relationships can help guid us in the parameterization and model selection in our ARMA models. 

```{r}
acf(ab.ts) # autocorrelation function; decays geometically
pacf(ab.ts) # partial autocorrelation function; decays geometrically
```



Together, the above analyses suggest that our data can most likely be fit with an additive model and that transformations or differencing is not required. 






Last, we will use an autoregressive integrated moving averge (ARIMA) approach to model our data.
ARIMA models predict a response variable based on regressing observations with past, lagged observations using linear comibiantion of error terms from the combinations of time points. 
ARIMA models accept parameters (*p*, *d*, and *q*) that describe the number of lags , differencing, and order of the moving-averge model, respectively. 
ARIMA models can also be used for forecasting into the future based on processes that fit the observed time series, as shown below. 
Obviously, the reliability of forecast diminshes with time as it's not being updated with new information. 

```{r}
ab.arm <- arima((ab.ts), c(0, 0, 1), seasonal = list(order = c(0, 1, 1), period = 2))
# arm models with intercept close to one suggest random walk, not arma
tsdiag(ab.arm)
pred.arm <- predict(ab.arm, n.ahead = 10*2)
ts.plot(ab.ts, pred.arm$pred, lty = c(1,3))
```

## 5) REPEATED MEASURES ANALYSIS OF VARIANCE (RM-ANOVA)
When scientists go to the trouble of setting up an experiment, whether it be in a laboratory, a hospital, or in the field, they tend to take more than one measurement on their experimental unit. 
Such studies are referred to as *longitudinal designs*.
Historically, there has been confusion about how to deal with the fact that these meausrements are non-indepdent and thus violate some of the major assumptions of most parameteric statistics. 
Repeated measures Analysis of Variance (ANOVA) provides on way of analyzing factorically designed longitudinal studies. 
In the following sectionn, we will show you how to peform and interpret a RM-ANOVA using data from the Portal Project. 

### Wrangle data for RM-ANOVA
Annual observed richness 
```{r}
# Contstruct time-by-species matrix
time.by.species <- group_by(portal, year, plot_id, plot_type) %>% count(taxon) %>% spread(key = taxon, value = n, fill = 0)

# Caclculate observed richness from time-by-species matrix
richness <- as.data.frame(rowSums(time.by.species[,-c(1,3)] > 0)) 

# Create dataframe with experimental design and richness data
rich.all <- data.frame(time.by.species[,1:3,], richness)

# Pull out two of the five treatments
rich.treat <- rich.all[which(rich.all$plot_type == "Control" | rich.all$plot_type == "Rodent Exclosure"), ]
names(rich.treat)[4] <- "richness"
#names(rich.treat)[names(rich.treat) == "rowSums.time.by.species....c.1..3..."] <- "richness"
```

### Plot data
In the following section, we'll return to the `group_by` function from `dplyr` along with pipes to retrieve the mean and standard deviation for the annual observed richness from the Control and Rodent Exclosure plots.
After that we will introduce a new way of plotting data in R.
While the base package in R can make beautiful plots and has a tremendous amount of flexibility, it can also require a lot of effort and code to generate figures. 
Another, perhaps easier, way to make figures in R is with the package `ggplot`. 
As you'll see below, with relatively few lines of code, we can make a nice-lookig plot with `ggplot` showing the effects of rodent exlcosure on mammal richness.  

```{r}
rich.treat.plot <- group_by(rich.treat, plot_type, year) %>%
  summarise(
    mean = mean(richness),   # avg. richness per group
    sd = sd(richness),       # stand. dev. per group
    n = n(),                 # num. obs. per group
    sem = sd/sqrt(n))        # calc. std. err. mean.

### NW: I used your sem function below (as well as one from Week1) and it doesn't work on this data set; or at least is generating lots of warnings; I wonder if this has somehting to do with unequal sample size? Maybe it's find to just leave it as standard deviaiton, but SEM is probably more appropriate. 

rich.plot <- ggplot(rich.treat.plot, aes(x = year, y = mean, color = plot_type)) +
  geom_line(size = 1, show.legend = T) + 
  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), width = .1) +
  xlim(1977, 2002) + 
  xlab("Year") + 
  ylab("Richness")

plot(rich.plot)
```

### Analyze the time series data with Repeated Measures Analysis of Variance (RM-ANOVA)

Background on RM-ANOVA. Why and when it's useful. 

Expand to talk about covrariance structure and how this ties in with ARIMA stuff above. 

Have them do AIC tests with other structures. 

Perhaps show them how to calculate LS-MEANS?
```{r}
rich.rm <- lme(richness ~ plot_type * year, random = ~ 1 | plot_id, 
            correlation=corAR1(form = ~ 1 | plot_id),
            data=rich.treat)

# We can use `summary()` to look at the output in detail
summary(rich.rm)

# We can also use `pander()` to make a cleaner looking table of output
set.caption("RMANOVA for Portal")
pander(anova(rich.rm))
```

### Some practice chunks to be eventually deleted
```{r}
# Some trials with calcuating abundance and richness
# time.by.species <- group_by(portal, year, plot_id) %>% count(taxon) %>% spread(key = taxon, value = n, fill = 0)
# filter(time.by.species, plot_id==2)
# abundance<-rowSums(filter(time.by.species, plot_id==2)[,-c(1,2)])
# richness<-rowSums(filter(time.by.species, plot_id==2)[,-c(1,2)]>0)
#filter(time.by.species, plot_type == "Control", plot_type == "Rodent Exclosure")

richness<-rowSums(filter(time.by.species, plot_type == "Control", plot_type == "Rodent Exclosure")[,-c(1,3)]>0)
# biomass <- group_by(portal, year, plot_id) %>% summarize(rod.mass = sum(weight), na.rm = TRUE)
# p2<-filter(biomass, plot_id > 2)
# plot(rod.mass ~ year, p2, xaxt = "n", type = "l")
# 
# port <- filter(portal, plot_id == 19)
# site2biomass <- group_by(portal, plot_id, year) %>%
#   summarise(
#     sum(na.omit(weight))
#   )
# plot(site2biomass, type = "l")
# plot(site2biomass[,1], site2biomass[,3])

# Richness plots by treatment

temp <- group_by(portal, plot_type, plot_id, year) %>% count(taxon) %>% spread(key = taxon, value = n, fill = 0)
div <- vegan::diversity(temp[,-c(1:3)], metric = "richness")
temp$div <- div
tempdiv <- group_by(temp, plot_type, year) %>%
  summarise(
    mean = mean(div), 
    sd = sd(div))

plots <- ggplot(tempdiv, aes(x = year, y = mean, color = plot_type)) +
  geom_line(size = 1, show.legend = T) + 
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = .1) +
  xlim(1977, 2002) + 
  xlab("Year") + 
  ylab("Div")
plot(plots)

# Abundance plots by treatment
temp <- group_by(portal, plot_type, plot_id, year) %>% count(taxon) %>% spread(key = taxon, value = n, fill = 0)
div <- rowSums(temp[,-c(1:3)])
temp$div <- div
tempdiv <- group_by(temp, plot_type, year) %>%
  summarise(
    mean = mean(div), 
    sd = sd(div)/sqrt(n()))

plots <- ggplot(tempdiv, aes(x = year, y = mean, color = plot_type)) +
  geom_line(size = 1, show.legend = T) + 
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = .1) +
  xlim(1977, 2002) + 
  xlab("Year") + 
  ylab("Div")
plot(plots)
```

## 6) TEMPORAL BETA DIVERSITY

The structure of an ecological community changes over time. 
We can conceptualize temporal turnover in much the same way as spatial turnover: how species change in abundance or presence over time. 


### A. Richness 
A simple measure of community change over time is how many species it gains or loses over the observed duration.
In the Chihuahan Desert, disturbances such as drought may influence which species are found in the experimental plots over time. 
In addition, some species may simply be transients that happened to be censused one year, but not in others.
The treatment design of the plots at the Portal site may influence which species are found in each group of sites. 
Before we proceed, we can generate a few hypotheses: 
* The control plots will have higher richness than the exclosure plots because many species are excluded from the exclosures.
* Alternatively, the kangaroo rat exclosure treatments may have moderately high diversity if the kangaroo rat tends to competitively displace other small mammals.
* Likewise, we can draw additional hypotheses about the exclosure of banner-tailed kangaroo rats (Spectab), and the exclosure of all rodents. 


First, we can calculate the richness in each plot, in each year. 
```{r}
portal.richness.year <- group_by(portal, year, plot_type) %>% 
  count(taxon) %>% 
  summarise(richness = n())
```

Now, let's plot these on the same graph to visualize
```{r, fig.align='center', fig.width=6, fig.height=4}
rich.plot <- ggplot(
  portal.richness.year, aes(x = year, y = richness, color = plot_type)) +  
  geom_line(size = 1, show.legend = T) + 
  xlim(1977, 2002) + 
  xlab("Year") + 
  ylab("Richness") + 
  theme(legend.position = "bottom")

plot(rich.plot)
```

It looks like the Control plots tend to have the highest richness, and had the highest peak richness.
It is also worth noting that the Krat exclosures and rodent exclosures remained relatively similar, but that the Spectab exclosure consistently had low richness.
There appears to have been an event that occurred around 1996 that led to a systematic crash in richness in all treatments, most severely in the exclosure treatments. 


### B. Turnover
Although we have just described general trends in richness, we have not yet learned anything about the change in species composition over time (i.e., species turnover).
Turnover gives us an idea of how similar species composition is over time: low turnover means that the composition and its relative abundances remain relatively stable through time, while high turnover suggests a highly dynamic community structure. 
Just like turnover in space, turnover can be driven by the introduction of new species or the loss of resident species.
Here we will calclulate the overall turnover, but also assess how this metric is influenced by the introduction of new species versus the disappearance of resident species. 

Total turnover = $\frac{\textrm{species gained + species lost}}{\textrm{total species in both timepoints}}$

To perform these analyses, we will use the `codyn` package (Hallett et al. 2016), which calculates a number of metrics to analyze community dynamics.
```{r}
# First, we will calculate the species abundances from each site over time
portal.species.abunds <- group_by(portal, year, plot_type) %>% count(taxon)

# This data.table now contains a new column `n` that represents the species counts


# Here, we calculate turnover 
portal.total <- turnover(df = portal.species.abunds, 
                            time.var = "year", 
                            species.var = "taxon", 
                            abundance.var = "n",
                            replicate.var = "plot_type",
                            metric = "total")

portal.appearance <- turnover(df = portal.species.abunds, 
                            time.var = "year", 
                            species.var = "taxon", 
                            abundance.var = "n",
                            replicate.var = "plot_type",
                            metric = "appearance")

portal.disappearance <- turnover(df = portal.species.abunds, 
                            time.var = "year", 
                            species.var = "taxon", 
                            abundance.var = "n",
                            replicate.var = "plot_type",
                            metric = "disappearance")
```
Each of these objects now contains a column for the value of the turnover metric, the second year in the pairwise comparison, and the type of plot. 
If we examine the data structure, note that the only difference between each object the column containing the calculation of the metric. 
For easier plotting, we'll combine these columns into a single data table

```{r}
# Let's join the columns by the shared year & plot type columns
portal.turnover <- full_join(portal.total, portal.disappearance) %>% 
  full_join(portal.appearance)

# Here, let's turn this back into long-form using gather
portal.turnover <- gather(portal.turnover, key = metric, value = turnover, 
       total, appearance, disappearance)
```

In the above code chunk, we did a lot of data wrangling using only a couple lines of code. 
Think about what we just did. 
We had three separate data frames containing different turnover metrics. 
But those metrics referred to the same plots and years. 
So, we joined them together using their shared year > plot_id index to create one data table.
We used a pipe `%>%` to first join total and disappearance values, then piped this joined data table to be the first argument in another join function. 
Run each of these `full_join()` statements independently to see what it is doing at each step.

Next, to facilitate plotting, we can turn this wide form data table back into a long form. 
We did this using the `gather()` function. 
This function says: in `portal.turnover`, create a new column of keys and call it "metric".
Create another new column of associated values and call it "turnover". 
The keys in the metric column will be the old columns we are gathering together (total, appearance, disappearance).
The values that go in turnover will be the values that were previously in the total, appearance, disappearance columns. 


Take a look at our new data table with the three different turnover metrics calculated for each year and treatment (try `View(portal.turnover)`). 

Let's visualize this.
```{r, fig.align='center', fig.height=6, fig.width=4}
turn.plot <- ggplot(
  portal.turnover, aes(x = year, y = turnover, color = metric)) +
  geom_line(size = 1, show.legend = T) + 
  facet_wrap(~plot_type, ncol = 1) +
  xlim(1977, 2002) + 
  xlab("Year") + 
  ylab("Turnover") + 
  theme(legend.position = "bottom")
plot(turn.plot)
```


### C. Rank Shift

```{r}
portal.rankshift <- rank_shift(df = portal.species.abunds, 
                               time.var = "year",
                               species.var = "taxon",
                               abundance.var = "n")
portal.rankshift$year <- as.numeric(substr(portal.rankshift$year_pair, 6, 9))

rankshift.plot <- ggplot(portal.rankshift, aes(x = year, y = MRS)) + 
  geom_line(size = 1) + 
  xlim(1977, 2002) + 
  xlab("Year") + 
  ylab("Mean Rank Shift")

plot(rankshift.plot)

# Check out help(rate_change_interval) and note that this function calculates the 
# Euclidean distance between each site. Remember, this is not an ideal distance.
# Let's use an appropriate transformation on the abundances first.

# let's cound the total abundances in each year (i.e., rowsum)
total.abunds <- portal.species.abunds %>% group_by(year) %>% count(wt = n)

# Now, let's join these total counts with the counts for each species
portal.hellinger.transf <- inner_join(portal.species.abunds, total.abunds, by = "year") %>% 
  mutate(hellinger.transf = sqrt(n / nn))

# Now, we have a third column, the square root of the relative abundances, the hellinger transformation
# calculating the euclidean distance on Hellinger-transformed data is appropriate.
portal.change.int <- rate_change_interval(portal.hellinger.transf,
                     time.var = "year",
                     species.var = "taxon",
                     abundance.var = "hellinger.transf")

rate.plot <- ggplot(portal.change.int, aes(interval, distance)) + 
  geom_point() + 
  stat_smooth(method = "loess", se = F, size = 1) + 
  ylab("Hellinger Distance") + 
  xlab("Time Interval (Years)")
rate.plot

turnover.grid.plot <- plot_grid(rich.plot, 
          turn.plot, 
          rankshift.plot,
          rate.plot,
          labels = c("A", "B", "C", "D"), 
          nrow = 4, align = "v")

ggsave("fig-turnover-grid.png", turnover.grid.plot,
       width = 6, height = 10, units = "in")
graphics.off()
grid::grid.raster(png::readPNG("fig-turnover-grid.png"))

```


## 7) SYNCHRONY, or covarying species; perhaps stability, compensatory dynamics, or variance ratio?
http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12188/epdf
https://cran.r-project.org/web/packages/vrtest/vrtest.pdf

## 8) Using environmental drivers to explain temporal biodiversity data (univariate, multivariate, both?)
## loading and plotting precipitation data
## perhaps take the sum rather than mean of precip below
```{r}
precip1 <- read.table(file = "./data/Portal_precipitation_19801989.csv", header = T, sep = ",")
precip2 <- read.table(file = "./data/Portal_precipitation_19892002.csv", header = T, sep = ",")

precip.month <- unite(precip1, col = date, c(Year, Month), sep = ".", remove = F)
precip.month$date <- as.Date(precip.month$date, "%Y-%m")
precip.month$date <- as.numeric(precip.month$date)

precip2 <- read.table(file = "./data/Portal_precipitation_19892002.csv", header = T, sep = ",")
precip2 <- unite(precip2, col = date, c(Year, Month, Day, Hour), sep = "-", remove = F)
parse_date_time(precip2$date, "%y-%m-%d-%H%M")

monthly.precip <- group_by(precip2, Year, Month) %>%
  summarise(mean = mean(Precipitation))
monthly.precip$date <- unite(monthly.precip, col = date, c(Year, Month), sep = ".")
plot(monthly.precip$date, monthly.precip$mean)
```